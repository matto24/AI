{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Monte Carlo approaches to prediction and control\n","\n","In this notebook, you will implement the Monte Carlo approaches to prediction and control described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lecture, but now without relying on knowledge of the task dynamics, that is, without relying on knowledge about transition probabilities."],"metadata":{"id":"WMapQn0-y2Bk"}},{"cell_type":"markdown","source":["### Install dependencies"],"metadata":{"id":"e9vApPmQFGUs"}},{"cell_type":"code","source":["! pip install numpy pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9pkLaPNMOTP","outputId":"e53cf2a6-b176-41e3-e411-4b883f448224","executionInfo":{"status":"ok","timestamp":1736265970320,"user_tz":-60,"elapsed":13863,"user":{"displayName":"Mads","userId":"12685562164969729722"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"Q8kdEPNmFOCr"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","import sys          # We use sys to get the max value of a float\n","import pandas as pd # We only use pandas for displaying tables nicely\n","pd.options.display.float_format = '{:,.3f}'.format"],"metadata":{"id":"T9cAvA0GLkXh","executionInfo":{"status":"ok","timestamp":1736265971620,"user_tz":-60,"elapsed":1303,"user":{"displayName":"Mads","userId":"12685562164969729722"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### ```World``` class and globals\n","\n","The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n","\n","A _state_ is a tuple $(x,y)$.\n","\n","An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n","\n","To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called.\n","\n","__Note that ```get_state_transition_probabilities``` has been removed and an agent must now rely on experience interacting with a world to learn.__"],"metadata":{"id":"lTNglEH9FR8f"}},{"cell_type":"code","source":["# Globals:\n","ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n","\n","# Rewards, terminals and obstacles are characters:\n","REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n","TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n","OBSTACLES = (\"#\")\n","\n","# Discount factor\n","gamma = 1\n","\n","# The probability of a random move:\n","rand_move_probability = 0\n","\n","class World:\n","  def __init__(self, width, height):\n","    self.width = width\n","    self.height = height\n","    # Create an empty world where the agent can move to all cells\n","    self.grid = np.full((width, height), ' ', dtype='U1')\n","\n","  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n","    \"\"\"\n","    Create an obstacle in either a single cell or rectangle.\n","    \"\"\"\n","    if end_x == None: end_x = start_x\n","    if end_y == None: end_y = start_y\n","\n","    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n","\n","  def add_reward(self, x, y, reward):\n","    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n","    self.grid[x, y] = reward\n","\n","  def add_terminal(self, x, y, terminal):\n","    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n","    self.grid[x, y] = terminal\n","\n","  def is_obstacle(self, x, y):\n","    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n","      return True\n","    else:\n","      return self.grid[x ,y] in OBSTACLES\n","\n","  def is_terminal(self, x, y):\n","    return self.grid[x ,y] in TERMINALS\n","\n","  def get_reward(self, x, y):\n","    \"\"\"\n","    Return the reward associated with a given location\n","    \"\"\"\n","    return REWARDS[self.grid[x, y]]\n","\n","  def get_next_state(self, current_state, action):\n","    \"\"\"\n","    Get the next state given a current state and an action. The outcome can be\n","    stochastic  where rand_move_probability determines the probability of\n","    ignoring the action and performing a random move.\n","    \"\"\"\n","    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n","\n","    x, y = current_state\n","\n","    # If our current state is a terminal, there is no next state\n","    if self.grid[x, y] in TERMINALS:\n","      return None\n","\n","    # Check of a random action should be performed:\n","    if np.random.rand() < rand_move_probability:\n","      action = np.random.choice(ACTIONS)\n","\n","    if action == \"up\":      y -= 1\n","    elif action == \"down\":  y += 1\n","    elif action == \"left\":  x -= 1\n","    elif action == \"right\": x += 1\n","\n","    # If the next state is an obstacle, stay in the current state\n","    return (x, y) if not self.is_obstacle(x, y) else current_state\n"],"metadata":{"id":"wMAd6qTASn9u","executionInfo":{"status":"ok","timestamp":1736265971620,"user_tz":-60,"elapsed":5,"user":{"displayName":"Mads","userId":"12685562164969729722"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Basic example: Generating episodes\n","\n","An episode is the series of states, actions and rewards reflecting an agent's experience interacting with the environment. An episode starts with an agent being placed at some initial state and continues till the agent reaches a terminal state.  To generate episodes, we first need a world and a policy:\n"],"metadata":{"id":"UxncUHvz1_Hb"}},{"cell_type":"code","source":["world = World(2, 3)\n","\n","# Since we only focus on episodic tasks, we must have a terminal state that the\n","# agent eventually reaches\n","world.add_terminal(1, 2, \"+\")\n","\n","def equiprobable_random_policy(x, y):\n","  return { k:1/len(ACTIONS) for k in ACTIONS }\n","\n","print(world.grid.T)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dL-XC6-aN6ss","outputId":"bc842a0f-31e7-4e96-831c-79dbc6b58c6b","executionInfo":{"status":"ok","timestamp":1736265971620,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mads","userId":"12685562164969729722"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[[' ' ' ']\n"," [' ' ' ']\n"," [' ' '+']]\n"]}]},{"cell_type":"markdown","source":["To generate an episode, we need to provide a ```World```, a policy, and a start state.\n","\n","In each step, we do the following:\n","1. perform one of the actions (weighted random) returned by the policy for the giving state\n","2. get the reward and add a new entry to the episode $[S_t, A_t, R_{t+1}]$\n","3. move the agent to the next state\n","\n","When a terminal state is reached, we return all the $[[S_0, A_0, R_1], ..., [S_{T}, A_T, R_{T+1}]]$ observed in the episode."],"metadata":{"id":"BfMg_Zjl40vx"}},{"cell_type":"code","source":["def generate_episode(world, policy, start_state):\n","    current_state = start_state\n","    episode = []\n","    while not world.is_terminal(*current_state):\n","        # Get the possible actions and their probabilities that our policy says\n","        # that the agent should perform in the current state:\n","        possible_actions = policy(*current_state)\n","\n","        # Pick a weighted random action:\n","        action = random.choices(population=list(possible_actions.keys()),\n","                                weights=possible_actions.values(), k=1)\n","\n","        # Get the next state from the world\n","        next_state = world.get_next_state(current_state, action[0])\n","\n","        # Get the reward for performing the action\n","        reward = world.get_reward(*next_state)\n","\n","        # Save the state, action and reward for this time step in our episode\n","        episode.append([current_state, action[0], reward])\n","\n","        # Move the agent to the new state\n","        current_state = next_state\n","\n","    return episode"],"metadata":{"id":"51rP-8eH4w1V","executionInfo":{"status":"ok","timestamp":1736265971620,"user_tz":-60,"elapsed":3,"user":{"displayName":"Mads","userId":"12685562164969729722"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Now, we can try to generate a couple of episodes and print the result:"],"metadata":{"id":"lTNH2PP06lBJ"}},{"cell_type":"code","source":["for i in range(5):\n","    print(f\"Episode {i}:\")\n","    episode = generate_episode(world, equiprobable_random_policy, (0, 0))\n","    print(pd.DataFrame(episode, columns=[\"State\", \"Action\", \"Reward\"]), end=\"\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PsyPGgMt2skf","outputId":"54ba21a0-5d3c-4029-d975-0b0e7aac1d57","executionInfo":{"status":"ok","timestamp":1736265972414,"user_tz":-60,"elapsed":796,"user":{"displayName":"Mads","userId":"12685562164969729722"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 0:\n","     State Action  Reward\n","0   (0, 0)     up       0\n","1   (0, 0)   left       0\n","2   (0, 0)  right       0\n","3   (1, 0)     up       0\n","4   (1, 0)  right       0\n","5   (1, 0)     up       0\n","6   (1, 0)   left       0\n","7   (0, 0)  right       0\n","8   (1, 0)   down       0\n","9   (1, 1)  right       0\n","10  (1, 1)   down      10\n","\n","Episode 1:\n","    State Action  Reward\n","0  (0, 0)  right       0\n","1  (1, 0)   down       0\n","2  (1, 1)     up       0\n","3  (1, 0)  right       0\n","4  (1, 0)     up       0\n","5  (1, 0)  right       0\n","6  (1, 0)   down       0\n","7  (1, 1)   down      10\n","\n","Episode 2:\n","    State Action  Reward\n","0  (0, 0)  right       0\n","1  (1, 0)  right       0\n","2  (1, 0)  right       0\n","3  (1, 0)   down       0\n","4  (1, 1)   left       0\n","5  (0, 1)     up       0\n","6  (0, 0)  right       0\n","7  (1, 0)   down       0\n","8  (1, 1)   down      10\n","\n","Episode 3:\n","     State Action  Reward\n","0   (0, 0)   left       0\n","1   (0, 0)     up       0\n","2   (0, 0)   down       0\n","3   (0, 1)     up       0\n","4   (0, 0)   down       0\n","5   (0, 1)     up       0\n","6   (0, 0)  right       0\n","7   (1, 0)   left       0\n","8   (0, 0)     up       0\n","9   (0, 0)   down       0\n","10  (0, 1)   down       0\n","11  (0, 2)  right      10\n","\n","Episode 4:\n","     State Action  Reward\n","0   (0, 0)     up       0\n","1   (0, 0)  right       0\n","2   (1, 0)     up       0\n","3   (1, 0)   left       0\n","4   (0, 0)  right       0\n","5   (1, 0)  right       0\n","6   (1, 0)   left       0\n","7   (0, 0)  right       0\n","8   (1, 0)  right       0\n","9   (1, 0)   down       0\n","10  (1, 1)   down      10\n","\n"]}]},{"cell_type":"markdown","source":["### Exercise: Implement Monte Carlo-based prediction for state values\n","\n","You should implement first-visit MC prediction for estimating $V≈v_\\pi$. See page 92 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n"],"metadata":{"id":"yLj3TWX3xDv-"}},{"cell_type":"code","source":["### TODO: Implement your code here\n","def mc_first_pred(world, policy):\n","  V = np.full((world.width, world.height), 0.0)\n","  sum_of_returns = np.full((world.width, world.height), 0.0)\n","  times_visited = np.full((world.width, world.height), 0.0)\n","\n","  for _ in range(10000):\n","    start_x = random.randint(0, world.width - 1)\n","    start_y = random.randint(0, world.height - 1)\n","    episode = generate_episode(world, policy, (start_x, start_y))\n","\n","    G = 0\n","    #For loop through episodes backwards as int\n","    for i in range(len(episode)-1, -1, -1):\n","      G = gamma*G+episode[i][2]\n","      isVisited = False\n","      for j in range(0, i-1):\n","        if episode[i][0] == episode[j][0]:\n","          isVisited = True\n","          break\n","      if not isVisited:\n","        sum_of_returns[episode[i][0]] += G\n","        times_visited[episode[i][0]] += 1\n","        V[episode[i][0]] = sum_of_returns[episode[i][0]]/times_visited[episode[i][0]]\n","\n","  return V\n","\n"],"metadata":{"id":"LuBuwnmZy8Dg","executionInfo":{"status":"ok","timestamp":1736265972414,"user_tz":-60,"elapsed":2,"user":{"displayName":"Mads","userId":"12685562164969729722"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["First, try your algorithm on the small $2\\times3$ world above using an equiprobable policy and $\\gamma = 0.9$. Depending on the number of episodes you use, you should get close to the true values:\n","\n","<table class=\"dataframe\" border=\"1\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.283</td>\n","      <td>3.616</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.409</td>\n","      <td>5.556</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6.349</td>\n","      <td>0.000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","\n","\n"],"metadata":{"id":"cSgpTNFBy8eq"}},{"cell_type":"code","source":["gamma = 0.9\n","\n","### TODO: Implement your code here\n","v1 = mc_first_pred(world, equiprobable_random_policy)\n","display(pd.DataFrame(v1.T))"],"metadata":{"id":"eRvNY8oR6tjs","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1736265975889,"user_tz":-60,"elapsed":3477,"user":{"displayName":"Mads","userId":"12685562164969729722"}},"outputId":"1f90ee46-a6ff-4055-fd18-391c3210f33d"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["      0     1\n","0 3.291 3.625\n","1 4.437 5.540\n","2 6.304 0.000"],"text/html":["\n","  <div id=\"df-08051c2c-7354-4b54-9e93-17b6d68c2d50\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.291</td>\n","      <td>3.625</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.437</td>\n","      <td>5.540</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6.304</td>\n","      <td>0.000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08051c2c-7354-4b54-9e93-17b6d68c2d50')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-08051c2c-7354-4b54-9e93-17b6d68c2d50 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-08051c2c-7354-4b54-9e93-17b6d68c2d50');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ab28f153-2776-40f4-8c4b-d5514b23701d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab28f153-2776-40f4-8c4b-d5514b23701d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ab28f153-2776-40f4-8c4b-d5514b23701d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5206871049808042,\n        \"min\": 3.2913443133787608,\n        \"max\": 6.304090501594699,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.2913443133787608,\n          4.437178757953215,\n          6.304090501594699\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.8134191510124307,\n        \"min\": 0.0,\n        \"max\": 5.539568376690966,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.6246927260409376,\n          5.539568376690966,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"markdown","source":["Try to run your MC prediction code on worlds of different sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long). You can try to change the policy as well, but rememeber that the agent **must** eventually reach a terminal state under any policy that you try."],"metadata":{"id":"1AxMw0aZ2I-9"}},{"cell_type":"code","source":["### TODO: Implement your code here\n","\n","world2 = World(10, 10)\n","world2.add_terminal(3, 3, \"+\")\n","world2.add_terminal(0, 0, \"+\")\n","v = mc_first_pred(world2, equiprobable_random_policy)\n","display(pd.DataFrame(v.T))"],"metadata":{"id":"yjORC8Zl2JWl","colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"status":"ok","timestamp":1736266269926,"user_tz":-60,"elapsed":32147,"user":{"displayName":"Mads","userId":"12685562164969729722"}},"outputId":"2edc86d2-5e1b-48a9-d232-983561598304"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["      0     1     2     3     4     5     6     7     8     9\n","0 0.000 4.948 2.639 1.745 1.185 0.720 0.436 0.268 0.149 0.111\n","1 5.102 3.504 2.537 2.246 1.552 0.922 0.533 0.304 0.156 0.112\n","2 2.750 2.585 3.065 4.354 2.455 1.240 0.638 0.310 0.163 0.108\n","3 1.778 2.292 4.193 0.000 3.920 1.596 0.702 0.323 0.165 0.111\n","4 1.224 1.553 2.471 3.996 2.303 1.158 0.577 0.288 0.143 0.101\n","5 0.729 0.887 1.272 1.659 1.195 0.695 0.380 0.212 0.123 0.088\n","6 0.440 0.495 0.609 0.675 0.559 0.380 0.237 0.145 0.082 0.066\n","7 0.231 0.262 0.292 0.303 0.268 0.207 0.133 0.091 0.063 0.048\n","8 0.138 0.162 0.169 0.153 0.137 0.117 0.091 0.063 0.042 0.032\n","9 0.104 0.109 0.118 0.109 0.089 0.085 0.062 0.046 0.031 0.027"],"text/html":["\n","  <div id=\"df-867c08ac-e831-472c-8a20-3299983a4933\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000</td>\n","      <td>4.948</td>\n","      <td>2.639</td>\n","      <td>1.745</td>\n","      <td>1.185</td>\n","      <td>0.720</td>\n","      <td>0.436</td>\n","      <td>0.268</td>\n","      <td>0.149</td>\n","      <td>0.111</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.102</td>\n","      <td>3.504</td>\n","      <td>2.537</td>\n","      <td>2.246</td>\n","      <td>1.552</td>\n","      <td>0.922</td>\n","      <td>0.533</td>\n","      <td>0.304</td>\n","      <td>0.156</td>\n","      <td>0.112</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.750</td>\n","      <td>2.585</td>\n","      <td>3.065</td>\n","      <td>4.354</td>\n","      <td>2.455</td>\n","      <td>1.240</td>\n","      <td>0.638</td>\n","      <td>0.310</td>\n","      <td>0.163</td>\n","      <td>0.108</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.778</td>\n","      <td>2.292</td>\n","      <td>4.193</td>\n","      <td>0.000</td>\n","      <td>3.920</td>\n","      <td>1.596</td>\n","      <td>0.702</td>\n","      <td>0.323</td>\n","      <td>0.165</td>\n","      <td>0.111</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.224</td>\n","      <td>1.553</td>\n","      <td>2.471</td>\n","      <td>3.996</td>\n","      <td>2.303</td>\n","      <td>1.158</td>\n","      <td>0.577</td>\n","      <td>0.288</td>\n","      <td>0.143</td>\n","      <td>0.101</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.729</td>\n","      <td>0.887</td>\n","      <td>1.272</td>\n","      <td>1.659</td>\n","      <td>1.195</td>\n","      <td>0.695</td>\n","      <td>0.380</td>\n","      <td>0.212</td>\n","      <td>0.123</td>\n","      <td>0.088</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.440</td>\n","      <td>0.495</td>\n","      <td>0.609</td>\n","      <td>0.675</td>\n","      <td>0.559</td>\n","      <td>0.380</td>\n","      <td>0.237</td>\n","      <td>0.145</td>\n","      <td>0.082</td>\n","      <td>0.066</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.231</td>\n","      <td>0.262</td>\n","      <td>0.292</td>\n","      <td>0.303</td>\n","      <td>0.268</td>\n","      <td>0.207</td>\n","      <td>0.133</td>\n","      <td>0.091</td>\n","      <td>0.063</td>\n","      <td>0.048</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.138</td>\n","      <td>0.162</td>\n","      <td>0.169</td>\n","      <td>0.153</td>\n","      <td>0.137</td>\n","      <td>0.117</td>\n","      <td>0.091</td>\n","      <td>0.063</td>\n","      <td>0.042</td>\n","      <td>0.032</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.104</td>\n","      <td>0.109</td>\n","      <td>0.118</td>\n","      <td>0.109</td>\n","      <td>0.089</td>\n","      <td>0.085</td>\n","      <td>0.062</td>\n","      <td>0.046</td>\n","      <td>0.031</td>\n","      <td>0.027</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-867c08ac-e831-472c-8a20-3299983a4933')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-867c08ac-e831-472c-8a20-3299983a4933 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-867c08ac-e831-472c-8a20-3299983a4933');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7d5b7e9b-dad3-497a-b339-29a4cd8f614c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d5b7e9b-dad3-497a-b339-29a4cd8f614c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7d5b7e9b-dad3-497a-b339-29a4cd8f614c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(pd\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6140620101921828,\n        \"min\": 0.0,\n        \"max\": 5.102237126910204,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.1378843116782033,\n          5.102237126910204,\n          0.7292574343697191\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6339691189383534,\n        \"min\": 0.10941425903600581,\n        \"max\": 4.947696879810502,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.16184349962226174,\n          3.503650907407345,\n          0.8872961164412412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4317083196467342,\n        \"min\": 0.11773805502961267,\n        \"max\": 4.193254078083028,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.16865350631063855,\n          2.5365003076157566,\n          1.2723147550903127\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6028413095592835,\n        \"min\": 0.0,\n        \"max\": 4.35433418976293,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.1533697587818435,\n          2.245675765186515,\n          1.6591186995143108\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.233110751984112,\n        \"min\": 0.08854910499585926,\n        \"max\": 3.9204299313852826,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.1368740678935743,\n          1.5524692432774818,\n          1.1954906284620745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5180595392629906,\n        \"min\": 0.08534129162462831,\n        \"max\": 1.5961795432964723,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.11737696663060732,\n          0.9216438666665678,\n          0.6951657145376269\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2360564798323742,\n        \"min\": 0.06246452971971158,\n        \"max\": 0.7018184721327267,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.09055873766261051,\n          0.5328211056589798,\n          0.37979411829867155\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10931865861002585,\n        \"min\": 0.04569442510893176,\n        \"max\": 0.3230093377712829,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.06345411660627524,\n          0.3038501570995341,\n          0.21196120278597066\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.052000467120523704,\n        \"min\": 0.031155496138375594,\n        \"max\": 0.16468818217483572,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.042374824236754115,\n          0.15577951193009631,\n          0.12300196525775775\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03430827372177126,\n        \"min\": 0.026795791639856188,\n        \"max\": 0.1119765944961634,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.03180631312939886,\n          0.1119765944961634,\n          0.08846198643381993\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"markdown","source":["### Exercise: Implement Monte Carlo-based prediction for state-action values\n","\n","There is one more step that has to be in place before we can start to optimize a policy: estimating state-action values, $q_\\pi(s,a)$, based on experience. Above where we estimated $v_\\pi$, we only needed to keep track of the average return observed for _each state_. However, in order to estimate state-action values, we need to compute the average return observed for _each state-action_ pair.\n","\n","That is, for every state $(0,0), (0,1), (0,2)...$ we need to compute different estimates for the four actions ```[ \"up\", \"down\", \"left\", \"right\" ]```"],"metadata":{"id":"BsCCnIH4z_g1"}},{"cell_type":"code","source":["### TODO: Implement your code here to predict state-action values.\n","\n","def mces(world):\n","  pol = np.full((world.width, world.height), None)\n","  q = np.full((world.width, world.height, 4), 0.0)\n","  sum_of_returns = np.full((world.width, world.height, 4), 0.0)\n","  times_visited = np.full((world.width, world.height, 4), 0.0)\n","\n","  action_to_index = {action: i for i, action in enumerate(ACTIONS)}\n","\n","  for _ in range(10000):\n","    start_x = random.randint(0, world.width - 1)\n","    start_y = random.randint(0, world.height - 1)\n","    episode = generate_episode(world, equiprobable_random_policy, (start_x, start_y))\n","    G = 0\n","    for i in range(len(episode)-1, -1, -1):\n","      G = gamma*G+episode[i][2]\n","      isVisited = False\n","      for j in range(0, i-1):\n","        if episode[i][0] == episode[j][0] and episode[i][1] == episode[j][1]:\n","          isVisited = True\n","          break\n","      action = action_to_index[episode[i][1]]\n","      if not isVisited:\n","        x,y = episode[i][0]\n","        sum_of_returns[x,y,action] += G\n","        times_visited[x,y,action] += 1\n","        q[x,y,action] = sum_of_returns[x,y,action]/times_visited[x,y,action]\n","        pol[x,y] = ACTIONS[np.argmax(q[x,y])]\n","  return pol\n","\n","\n"],"metadata":{"id":"5ejjYJ65sQKW","executionInfo":{"status":"ok","timestamp":1736266018639,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mads","userId":"12685562164969729722"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Try to experiment with your implementation by running it on different world sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long), and try to experiment with different numbers of episodes:"],"metadata":{"id":"Q4qrvNKClqpc"}},{"cell_type":"code","source":["### TODO: Implement your code here\n","\n","pol = mces(world)\n","print(pol.T)"],"metadata":{"id":"E1f_Om_aOmVj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736266020066,"user_tz":-60,"elapsed":1430,"user":{"displayName":"Mads","userId":"12685562164969729722"}},"outputId":"b4c87064-db49-406f-d34a-78ab8f149255"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[['down' 'down']\n"," ['down' 'down']\n"," ['right' None]]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"ab5fWwOKBNhx"}},{"cell_type":"markdown","source":["### Exercise: Implement on-policy Monte Carlo-based control with an $\\epsilon$-soft policy\n","\n","You are now ready to implement MC-based control (see page 101 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the algorithm).\n","\n","In your implementation, you need to update the state-action estimates like in the exercise above, but now, you also need implement an $ϵ$-soft policy that you can modify. How could you do that?\n","\n","_Hint_: You can either represent your policy explicitly. That is, for each state $(x,y)$ you have a ```dict``` with actions and their probabilities which you then update each time you step through an episode. When the policy is called, it then just returns the ```dict``` with action probablities corresponding to the current state.\n","\n","Alternatively, you can compute the action probabilities when your policy is called based on the current action-values estimates."],"metadata":{"id":"fU4gAj-OThu9"}},{"cell_type":"code","source":["gamma = 0.9\n","epsilon = 0.1\n","\n","### TODO: Implement you code here. You need to define a policy function\n","###       and then the actual algorithm that goes through time step in each\n","###       episode and updates state-action values and the policy. Also, make\n","###       sure that you can print out the policy learned, that is, the action\n","###       with the highest expected value in each state.\n","\n","\n","### TODO: Implement your code here to predict state-action values.\n","\n","\n","sum_of_returns_soft = np.full((world.width, world.height, 4), 0.0)\n","times_visited_soft = np.full((world.width, world.height, 4), 1.0)\n","\n","def mc_soft_policy(x,y):\n","\n","  Q = []\n","  for a in ACTIONS:\n","    Q.append(sum_of_returns_soft[x,y,ACTIONS.index(a)] / times_visited_soft[x,y,ACTIONS.index(a)])\n","\n","  best_action = ACTIONS[np.argmax(Q)]\n","  if(np.random.rand() > epsilon ):\n","    return {best_action:1}\n","  else:\n","    return {k:1/len(ACTIONS) for k in ACTIONS}\n","\n","\n","\n","\n","\n","\n","def mc_control(world):\n","  pol = np.full((world.width, world.height), None)\n","  q = np.full((world.width, world.height, 4), 0.0)\n","  sum_of_returns = np.full((world.width, world.height, 4), 0.0)\n","  times_visited = np.full((world.width, world.height, 4), 0.0)\n","\n","  action_to_index = {action: i for i, action in enumerate(ACTIONS)}\n","\n","  for _ in range(10000):\n","    start_x = random.randint(0, world.width - 1)\n","    start_y = random.randint(0, world.height - 1)\n","    episode = generate_episode(world, mc_soft_policy, (start_x, start_y))\n","    G = 0\n","    for i in range(len(episode)-1, -1, -1):\n","      G = gamma*G+episode[i][2]\n","      isVisited = False\n","      for j in range(0, i-1):\n","        if episode[i][0] == episode[j][0] and episode[i][1] == episode[j][1]:\n","          isVisited = True\n","          break\n","      action = action_to_index[episode[i][1]]\n","      if not isVisited:\n","        x,y = episode[i][0]\n","        sum_of_returns[x,y,action] += G\n","        times_visited[x,y,action] += 1\n","        q[x,y,action] = sum_of_returns[x,y,action]/times_visited[x,y,action]\n","        pol[x,y] = ACTIONS[np.argmax(q[x,y])]\n","  return pol\n","\n","\n","\n","world3 = World(10, 10)\n","world3.add_terminal(3, 3, \"+\")\n","world3.add_terminal(0, 0, \"+\")\n","nypol = mc_control(world3)\n","print(nypol.T)\n"],"metadata":{"id":"fYJpKFy82PId","executionInfo":{"status":"error","timestamp":1736267243229,"user_tz":-60,"elapsed":230,"user":{"displayName":"Mads","userId":"12685562164969729722"}},"colab":{"base_uri":"https://localhost:8080/","height":347},"outputId":"d74a5b83-1e51-49c3-fa45-52c5dd38f803"},"execution_count":25,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index 2 is out of bounds for axis 0 with size 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-1486ce811529>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mworld3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mworld3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mnypol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnypol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-1486ce811529>\u001b[0m in \u001b[0;36mmc_control\u001b[0;34m(world)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mstart_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mstart_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_soft_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-866c8bbb8418>\u001b[0m in \u001b[0;36mgenerate_episode\u001b[0;34m(world, policy, start_state)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Get the possible actions and their probabilities that our policy says\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# that the agent should perform in the current state:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpossible_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Pick a weighted random action:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-1486ce811529>\u001b[0m in \u001b[0;36mmc_soft_policy\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_of_returns_soft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtimes_visited_soft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"]}]},{"cell_type":"markdown","source":["Try to experiment with your implementation by running it on different world sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long), try to experiment with different numbers of episodes, and different values of epsilon:"],"metadata":{"id":"O7KVqyXJnMo6"}},{"cell_type":"code","source":["### TODO: Implement your code here\n"],"metadata":{"id":"ppZT2RlwcsXM","colab":{"base_uri":"https://localhost:8080/","height":347},"executionInfo":{"status":"error","timestamp":1736267052756,"user_tz":-60,"elapsed":214,"user":{"displayName":"Mads","userId":"12685562164969729722"}},"outputId":"dba9b638-0949-4c8d-fba8-f47b8df460aa"},"execution_count":21,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index 4 is out of bounds for axis 0 with size 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-0dfe8192b823>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mworld3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mworld3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnypol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnypol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-284dbafc93f1>\u001b[0m in \u001b[0;36mmc_control\u001b[0;34m(world)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mstart_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mstart_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_soft_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-866c8bbb8418>\u001b[0m in \u001b[0;36mgenerate_episode\u001b[0;34m(world, policy, start_state)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Get the possible actions and their probabilities that our policy says\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# that the agent should perform in the current state:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpossible_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Pick a weighted random action:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-284dbafc93f1>\u001b[0m in \u001b[0;36mmc_soft_policy\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_of_returns_soft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtimes_visited_soft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 2"]}]},{"cell_type":"markdown","source":["### Optional exercise\n","\n","Try to implement exploring starts (see page 99 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the algorithm). It should be straightforward and only require minimal changes to the code for the exercise above."],"metadata":{"id":"qFdgcLeDnZre"}}]}